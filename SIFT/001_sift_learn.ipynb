{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class ImageStitching():\n",
    "    def __init__(self):\n",
    "        self.ratio = 0.75  # Adjusted ratio for better matching\n",
    "        self.min_match = 10\n",
    "        self.orb = cv2.ORB_create()\n",
    "        self.smoothing_window_size = 800\n",
    "\n",
    "    def registration(self, img1, img2):\n",
    "        # Detect keypoints and descriptors\n",
    "        kp1, des1 = self.orb.detectAndCompute(img1, None)\n",
    "        kp2, des2 = self.orb.detectAndCompute(img2, None)\n",
    "        \n",
    "        # Match descriptors using BFMatcher with Hamming distance\n",
    "        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "        matches = matcher.match(des1, des2)\n",
    "        matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "        # Filter good matches based on the ratio\n",
    "        good_matches = [m for m in matches if m.distance < self.ratio * matches[-1].distance]\n",
    "\n",
    "        # Draw matches for visualization\n",
    "        img3 = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, flags=2)\n",
    "        cv2.imwrite('matching.jpg', img3)\n",
    "\n",
    "        # Compute homography if there are enough good matches\n",
    "        if len(good_matches) > self.min_match:\n",
    "            image1_kp = np.float32([kp1[m.queryIdx].pt for m in good_matches])\n",
    "            image2_kp = np.float32([kp2[m.trainIdx].pt for m in good_matches])\n",
    "            H, status = cv2.findHomography(image2_kp, image1_kp, cv2.RANSAC, 5.0)\n",
    "            return H\n",
    "        else:\n",
    "            raise ValueError(\"Not enough matches found for homography estimation.\")\n",
    "    \n",
    "    def create_mask(self, img1, img2, version):\n",
    "        height_img1, width_img1 = img1.shape[:2]\n",
    "        width_img2 = img2.shape[1]\n",
    "        height_panorama = height_img1\n",
    "        width_panorama = width_img1 + width_img2\n",
    "        offset = int(self.smoothing_window_size / 2)\n",
    "        barrier = width_img1 - offset\n",
    "\n",
    "        mask = np.zeros((height_panorama, width_panorama))\n",
    "        if version == 'left_image':\n",
    "            mask[:, barrier - offset:barrier + offset] = np.tile(np.linspace(1, 0, 2 * offset), (height_panorama, 1))\n",
    "            mask[:, :barrier - offset] = 1\n",
    "        elif version == 'right_image':\n",
    "            mask[:, barrier - offset:barrier + offset] = np.tile(np.linspace(0, 1, 2 * offset), (height_panorama, 1))\n",
    "            mask[:, barrier + offset:] = 1\n",
    "        return cv2.merge([mask, mask, mask])\n",
    "    \n",
    "    def blending(self, img1, img2):\n",
    "        # Perform image registration to find homography\n",
    "        H = self.registration(img1, img2)\n",
    "\n",
    "        # Image dimensions\n",
    "        height_img1, width_img1 = img1.shape[:2]\n",
    "        width_img2 = img2.shape[1]\n",
    "        height_panorama = height_img1\n",
    "        width_panorama = width_img1 + width_img2\n",
    "\n",
    "        # Prepare the panorama with initial image and mask\n",
    "        panorama1 = np.zeros((height_panorama, width_panorama, 3))\n",
    "        mask1 = self.create_mask(img1, img2, version='left_image')\n",
    "        panorama1[0:height_img1, 0:width_img1] = img1\n",
    "        panorama1 *= mask1\n",
    "\n",
    "        # Warp second image and apply the mask\n",
    "        mask2 = self.create_mask(img1, img2, version='right_image')\n",
    "        panorama2 = cv2.warpPerspective(img2, H, (width_panorama, height_panorama)) * mask2\n",
    "        \n",
    "        # Combine the two panoramas\n",
    "        result = panorama1 + panorama2\n",
    "\n",
    "        # Crop the final result to remove black borders\n",
    "        rows, cols = np.where(result[:, :, 0] != 0)\n",
    "        min_row, max_row = min(rows), max(rows) + 1\n",
    "        min_col, max_col = min(cols), max(cols) + 1\n",
    "        final_result = result[min_row:max_row, min_col:max_col]\n",
    "        return final_result\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'H' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m img1 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(argv1)\n\u001b[1;32m      5\u001b[0m img2 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(argv2)\n\u001b[0;32m----> 6\u001b[0m final\u001b[38;5;241m=\u001b[39m\u001b[43mImage_Stitching\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblending\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpanorama.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, final)\n",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m, in \u001b[0;36mImage_Stitching.blending\u001b[0;34m(self, img1, img2)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mblending\u001b[39m(\u001b[38;5;28mself\u001b[39m, img1, img2):\n\u001b[0;32m---> 50\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     height_img1 \u001b[38;5;241m=\u001b[39m img1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     52\u001b[0m     width_img1 \u001b[38;5;241m=\u001b[39m img1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m, in \u001b[0;36mImage_Stitching.registration\u001b[0;34m(self, img1, img2)\u001b[0m\n\u001b[1;32m     28\u001b[0m     image2_kp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32([kp2[m\u001b[38;5;241m.\u001b[39mtrainIdx]\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m good_matches])\n\u001b[1;32m     29\u001b[0m     H, status \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mfindHomography(image2_kp, image1_kp, cv2\u001b[38;5;241m.\u001b[39mRANSAC, \u001b[38;5;241m5.0\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mH\u001b[49m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'H' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "argv1 = '../data/undistorted_image.png'\n",
    "argv2 = '../data/undistorted_image2.png'\n",
    "\n",
    "img1 = cv2.imread(argv1)\n",
    "img2 = cv2.imread(argv2)\n",
    "final=Image_Stitching().blending(img1,img2)\n",
    "cv2.imwrite('panorama.jpg', final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_distortion(img, dist_coeffs= [-1, 1, 0, 0]):\n",
    "    \"\"\"\n",
    "    校正图像畸变的函数。\n",
    "\n",
    "    参数：\n",
    "    img (numpy.ndarray): 输入图像\n",
    "    K (numpy.ndarray): 相机内参矩阵\n",
    "    dist_coeffs (numpy.ndarray): 畸变系数\n",
    "\n",
    "    返回：\n",
    "    numpy.ndarray: 校正后的图像\n",
    "    \"\"\"\n",
    "    # 假设相机内参矩阵（需要根据具体相机参数调整）\n",
    "    K = np.array([[800, 0, img.shape[1] / 2],\n",
    "                  [0, 800, img.shape[0] / 2],\n",
    "                  [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "    # 定义畸变系数\n",
    "    dist_coeffs = np.array(dist_coeffs)\n",
    "    # 计算新的相机矩阵\n",
    "    new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K, dist_coeffs, (img.shape[1], img.shape[0]), 1, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # 畸变校正\n",
    "    corrected_img = cv2.undistort(img, K, dist_coeffs, None, new_camera_matrix)\n",
    "\n",
    "    # 裁剪图像\n",
    "    x, y, w, h = roi\n",
    "    corrected_img = corrected_img[y:y+h, x:x+w]\n",
    "\n",
    "    return corrected_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im11 = correct_distortion(img1)\n",
    "cv2.imwrite(\"../data/video1_frame1_d.jpg\", im11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('../data/video1_frame1_d.jpg', correct_distortion(img1))\n",
    "cv2.imwrite('../data/video2_frame1_d.jpg', correct_distortion(img2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def perspective_correction(image_path, output_path, points):\n",
    "    # 读取图像\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # 获取图像尺寸\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # 指定输入图像的四个角点\n",
    "    pts1 = np.float32(points)\n",
    "\n",
    "    # 计算透视变换矩阵\n",
    "    matrix = cv2.getPerspectiveTransform(pts1, np.float32([[0, 0], [1, 0], [1, 1], [0, 1]]))\n",
    "\n",
    "    # 应用透视变换来找到新的边界框\n",
    "    transformed_points = cv2.perspectiveTransform(np.array([points], dtype=np.float32), matrix)\n",
    "    transformed_points = transformed_points[0]\n",
    "\n",
    "    # 计算新图像的边界\n",
    "    min_x = np.min(transformed_points[:, 0])\n",
    "    max_x = np.max(transformed_points[:, 0])\n",
    "    min_y = np.min(transformed_points[:, 1])\n",
    "    max_y = np.max(transformed_points[:, 1])\n",
    "\n",
    "    # 新图像的尺寸\n",
    "    new_w = int(max_x - min_x)\n",
    "    new_h = int(max_y - min_y)\n",
    "\n",
    "    # 调整输出的四个角点\n",
    "    pts2 = np.float32([\n",
    "        [-min_x, -min_y],\n",
    "        [new_w - min_x, -min_y],\n",
    "        [new_w - min_x, new_h - min_y],\n",
    "        [-min_x, new_h - min_y]\n",
    "    ])\n",
    "\n",
    "    # 重新计算透视变换矩阵\n",
    "    matrix = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "\n",
    "    # 进行透视变换\n",
    "    result = cv2.warpPerspective(img, matrix, (new_w, new_h))\n",
    "\n",
    "    # 保存输出图像\n",
    "    cv2.imwrite(output_path, result)\n",
    "\n",
    "# 使用示例\n",
    "# 输入图像四个角点的坐标 (根据实际情况调整)\n",
    "points = [[100, 100], [500, 100], [500, 500], [100, 500]]\n",
    "perspective_correction('../data/video1_frame1.jpg', 'output.jpg', points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ziyiv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
